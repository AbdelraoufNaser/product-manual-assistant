{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy2rb6DtuYsz"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Required Dependencies (NO GOOGLE PACKAGES)\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentence-transformers\n",
        "!pip install qdrant-client\n",
        "!pip install pypdf\n",
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Required Libraries\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "import pdfplumber\n",
        "\n",
        "# Transformers and embeddings\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Vector operations\n",
        "import numpy as np\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ],
      "metadata": {
        "id": "OI_vpTpKu-8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Cell 3: Configure Hugging Face Models\n",
        "print(\"Setting up Hugging Face models...\")\n",
        "\n",
        "# Initialize question-answering model (best for manual queries)\n",
        "try:\n",
        "    qa_pipeline = pipeline(\n",
        "        \"question-answering\",\n",
        "        model=\"distilbert-base-cased-distilled-squad\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    print(\"Question-answering model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Q&A model: {e}\")\n",
        "\n",
        "# Initialize embeddings model\n",
        "try:\n",
        "    embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    print(\"Embeddings model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "\n",
        "print(\"Models configured successfully!\")\n"
      ],
      "metadata": {
        "id": "zV9B4kEgvMv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Document Processing Functions\n",
        "def extract_section_heading(text: str, page_num: int) -> str:\n",
        "    \"\"\"Extract section heading from document text\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines[:10]:\n",
        "        line = line.strip()\n",
        "        if line and len(line) < 100 and (line.isupper() or line.istitle()):\n",
        "            return line\n",
        "    return f\"Page {page_num}\"\n",
        "\n",
        "def try_multiple_pdf_loaders(pdf_path: str):\n",
        "    \"\"\"Try multiple PDF loading strategies\"\"\"\n",
        "    # Strategy 1: PyPDF2\n",
        "    try:\n",
        "        docs = []\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num, page in enumerate(pdf_reader.pages):\n",
        "                try:\n",
        "                    text = page.extract_text()\n",
        "                    if text.strip():\n",
        "                        docs.append({\n",
        "                            'page_content': text,\n",
        "                            'metadata': {\n",
        "                                'page': page_num + 1,\n",
        "                                'source': pdf_path,\n",
        "                                'document_name': Path(pdf_path).stem\n",
        "                            }\n",
        "                        })\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if docs:\n",
        "            print(f\"Successfully loaded {pdf_path} with PyPDF2\")\n",
        "            return docs\n",
        "    except Exception as e:\n",
        "        print(f\"PyPDF2 failed for {pdf_path}: {str(e)}\")\n",
        "\n",
        "    # Strategy 2: pdfplumber\n",
        "    try:\n",
        "        docs = []\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                try:\n",
        "                    text = page.extract_text()\n",
        "                    if text and text.strip():\n",
        "                        docs.append({\n",
        "                            'page_content': text,\n",
        "                            'metadata': {\n",
        "                                'page': page_num + 1,\n",
        "                                'source': pdf_path,\n",
        "                                'document_name': Path(pdf_path).stem\n",
        "                            }\n",
        "                        })\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if docs:\n",
        "            print(f\"Successfully loaded {pdf_path} with pdfplumber\")\n",
        "            return docs\n",
        "    except Exception as e:\n",
        "        print(f\"pdfplumber failed for {pdf_path}: {str(e)}\")\n",
        "\n",
        "    # If all fail, create placeholder\n",
        "    print(f\"All methods failed for {pdf_path}\")\n",
        "    return [{\n",
        "        'page_content': f\"Error loading PDF: {pdf_path}\",\n",
        "        'metadata': {'page': 1, 'source': pdf_path, 'error': True}\n",
        "    }]\n",
        "\n",
        "def load_and_process_pdfs(pdf_paths: List[str]) -> List[Dict]:\n",
        "    \"\"\"Load PDFs with multiple fallback strategies\"\"\"\n",
        "    all_documents = []\n",
        "    successful_loads = 0\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        print(f\"Processing: {pdf_path}\")\n",
        "        docs = try_multiple_pdf_loaders(pdf_path)\n",
        "\n",
        "        if docs and not docs[0]['metadata'].get('error', False):\n",
        "            successful_loads += 1\n",
        "\n",
        "        doc_name = Path(pdf_path).stem\n",
        "\n",
        "        for doc in docs:\n",
        "            page_num = doc['metadata'].get('page', 0)\n",
        "            section_heading = extract_section_heading(doc['page_content'], page_num)\n",
        "\n",
        "            doc['metadata'].update({\n",
        "                'document_name': doc_name,\n",
        "                'page_number': page_num,\n",
        "                'section_heading': section_heading,\n",
        "                'source_file': pdf_path,\n",
        "                'processed_date': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            all_documents.append(doc)\n",
        "\n",
        "    print(f\"Processing complete: {successful_loads}/{len(pdf_paths)} documents loaded\")\n",
        "    print(f\"Total pages: {len(all_documents)}\")\n",
        "\n",
        "    return all_documents\n",
        "\n",
        "def create_semantic_chunks(documents: List[Dict], chunk_size: int = 800, overlap: int = 150) -> List[Dict]:\n",
        "    \"\"\"Create semantic chunks\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        content = doc['page_content']\n",
        "\n",
        "        # Simple sentence-based chunking\n",
        "        sentences = content.split('. ')\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk + sentence) < chunk_size:\n",
        "                current_chunk += sentence + \". \"\n",
        "            else:\n",
        "                if current_chunk.strip():\n",
        "                    chunks.append({\n",
        "                        'page_content': current_chunk.strip(),\n",
        "                        'metadata': {\n",
        "                            **doc['metadata'],\n",
        "                            'chunk_id': len(chunks),\n",
        "                            'chunk_length': len(current_chunk),\n",
        "                            'chunk_type': 'sentence-based'\n",
        "                        }\n",
        "                    })\n",
        "                current_chunk = sentence + \". \"\n",
        "\n",
        "        # Add remaining chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'page_content': current_chunk.strip(),\n",
        "                'metadata': {\n",
        "                    **doc['metadata'],\n",
        "                    'chunk_id': len(chunks),\n",
        "                    'chunk_length': len(current_chunk),\n",
        "                    'chunk_type': 'sentence-based'\n",
        "                }\n",
        "            })\n",
        "\n",
        "    print(f\"Created {len(chunks)} semantic chunks\")\n",
        "    return chunks\n",
        "\n",
        "print(\"Document processing functions defined!\")"
      ],
      "metadata": {
        "id": "td7ql5M-vSKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Simple Vector Store Implementation\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Simple vector store using numpy for similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "\n",
        "    def add_documents(self, chunks: List[Dict]):\n",
        "        \"\"\"Add documents to vector store\"\"\"\n",
        "        print(f\"Adding {len(chunks)} chunks to vector store...\")\n",
        "\n",
        "        self.documents = chunks\n",
        "        contents = [chunk['page_content'] for chunk in chunks]\n",
        "\n",
        "        # Create embeddings\n",
        "        self.embeddings = self.embedding_model.encode(contents, show_progress_bar=True)\n",
        "\n",
        "        print(f\"Vector store setup complete with {len(chunks)} documents\")\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        if not self.documents:\n",
        "            return []\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
        "\n",
        "        # Get top k results\n",
        "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                'page_content': self.documents[idx]['page_content'],\n",
        "                'metadata': self.documents[idx]['metadata'],\n",
        "                'similarity_score': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"Vector store functions defined!\")"
      ],
      "metadata": {
        "id": "J9gGDeZevWZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Hugging Face Chatbot with Memory\n",
        "class HuggingFaceChatbot:\n",
        "    \"\"\"Chatbot using Hugging Face models with conversational memory\"\"\"\n",
        "\n",
        "    def __init__(self, qa_pipeline, vector_store):\n",
        "        self.qa_pipeline = qa_pipeline\n",
        "        self.vector_store = vector_store\n",
        "        self.conversation_history = {}\n",
        "\n",
        "    def _format_context(self, docs: List[Dict]) -> str:\n",
        "        \"\"\"Format retrieved documents as context\"\"\"\n",
        "        context_parts = []\n",
        "        for i, doc in enumerate(docs, 1):\n",
        "            metadata = doc['metadata']\n",
        "            content = doc['page_content'][:500] + \"...\" if len(doc['page_content']) > 500 else doc['page_content']\n",
        "\n",
        "            context_part = f\"\"\"Document {i}:\n",
        "Source: {metadata.get('document_name', 'Unknown')}\n",
        "Page: {metadata.get('page_number', 'Unknown')}\n",
        "Content: {content}\n",
        "---\"\"\"\n",
        "            context_parts.append(context_part)\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def chat(self, session_id: str, user_input: str, show_sources: bool = False) -> Dict[str, Any]:\n",
        "        \"\"\"Main chat function\"\"\"\n",
        "\n",
        "        # Initialize session history\n",
        "        if session_id not in self.conversation_history:\n",
        "            self.conversation_history[session_id] = []\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = self.vector_store.similarity_search(user_input, k=3)\n",
        "\n",
        "        if docs:\n",
        "            # Use Q&A pipeline with retrieved context\n",
        "            context = self._format_context(docs)\n",
        "\n",
        "            try:\n",
        "                # Use the best document as context for Q&A\n",
        "                best_doc_content = docs[0]['page_content']\n",
        "\n",
        "                result = self.qa_pipeline(\n",
        "                    question=user_input,\n",
        "                    context=best_doc_content[:2000]  # Limit context length\n",
        "                )\n",
        "\n",
        "                answer = result['answer']\n",
        "                confidence = result.get('score', 0)\n",
        "\n",
        "                # Format response based on confidence\n",
        "                if confidence > 0.5:\n",
        "                    response = f\"{answer}\"\n",
        "                elif confidence > 0.2:\n",
        "                    response = f\"{answer} (Note: This answer has moderate confidence - please verify in the manual)\"\n",
        "                else:\n",
        "                    response = \"I found some relevant information but couldn't provide a confident answer. Please check the manual sections I found.\"\n",
        "\n",
        "                # Add source information\n",
        "                source_info = f\" (Source: {docs[0]['metadata'].get('document_name', 'Unknown')}, Page {docs[0]['metadata'].get('page_number', 'Unknown')})\"\n",
        "                response += source_info\n",
        "\n",
        "            except Exception as e:\n",
        "                response = f\"I found relevant manual sections but encountered an error processing your question. Please check the retrieved sections directly.\"\n",
        "        else:\n",
        "            # No relevant documents found\n",
        "            response = \"I couldn't find relevant information in the available manuals for your question. Please try rephrasing or ask about Samsung Galaxy S23, Canon EOS Rebel T7, or Whirlpool washing machine.\"\n",
        "\n",
        "        # Store conversation\n",
        "        self.conversation_history[session_id].append({\n",
        "            'question': user_input,\n",
        "            'response': response,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        # Prepare result\n",
        "        result = {\n",
        "            \"response\": response,\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"document\": doc['metadata'].get('document_name', 'Unknown'),\n",
        "                    \"page\": doc['metadata'].get('page_number', 'Unknown'),\n",
        "                    \"section\": doc['metadata'].get('section_heading', 'Unknown'),\n",
        "                    \"content_preview\": doc['page_content'][:200] + \"...\",\n",
        "                    \"similarity\": doc.get('similarity_score', 0)\n",
        "                }\n",
        "                for doc in docs\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if show_sources:\n",
        "            print(f\"Retrieved {len(docs)} relevant chunks:\")\n",
        "            for i, source in enumerate(result[\"sources\"], 1):\n",
        "                print(f\"{i}. {source['document']} (Page {source['page']})\")\n",
        "                print(f\"   Section: {source['section']}\")\n",
        "                print(f\"   Similarity: {source['similarity']:.3f}\")\n",
        "                print(f\"   Preview: {source['content_preview']}\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_session_summary(self, session_id: str) -> str:\n",
        "        \"\"\"Get summary of conversation history\"\"\"\n",
        "        if session_id not in self.conversation_history:\n",
        "            return \"No conversation history found.\"\n",
        "\n",
        "        history = self.conversation_history[session_id]\n",
        "        if not history:\n",
        "            return \"No messages in this session.\"\n",
        "\n",
        "        return f\"Session has {len(history)} messages. Last message: {history[-1]['question'][:100]}...\"\n",
        "\n",
        "    def clear_session(self, session_id: str):\n",
        "        \"\"\"Clear conversation history\"\"\"\n",
        "        if session_id in self.conversation_history:\n",
        "            del self.conversation_history[session_id]\n",
        "            print(f\"Session {session_id} cleared.\")\n",
        "\n",
        "print(\"Hugging Face chatbot class defined!\")"
      ],
      "metadata": {
        "id": "Co9lG9szvZRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: File Upload and Processing\n",
        "from google.colab import files\n",
        "\n",
        "def upload_and_process_manuals():\n",
        "    \"\"\"Handle file upload in Colab and process manuals\"\"\"\n",
        "    print(\"Please upload your PDF manual files...\")\n",
        "    print(\"Expected files:\")\n",
        "    print(\"- Samsung Galaxy S23 manual\")\n",
        "    print(\"- Canon EOS Rebel T7 manual\")\n",
        "    print(\"- Whirlpool Washing Machine manual\")\n",
        "    print(\"- Any additional product manual (optional)\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.pdf'):\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"Uploaded: {filename}\")\n",
        "\n",
        "    if len(pdf_paths) < 1:\n",
        "        print(\"No PDF files uploaded. Please upload at least one PDF file.\")\n",
        "        return []\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "# For testing purposes, if files are already in directory\n",
        "def get_existing_pdfs():\n",
        "    \"\"\"Get existing PDF files in current directory\"\"\"\n",
        "    pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "    print(f\"Found {len(pdf_files)} PDF files: {pdf_files}\")\n",
        "    return pdf_files\n",
        "\n",
        "# Try to get existing PDFs first, then upload if none found\n",
        "try:\n",
        "    manual_paths = get_existing_pdfs()\n",
        "    if not manual_paths:\n",
        "        print(\"No existing PDFs found. Please upload files.\")\n",
        "        manual_paths = upload_and_process_manuals()\n",
        "except:\n",
        "    print(\"Please upload your PDF manual files using the upload function.\")\n",
        "    manual_paths = []"
      ],
      "metadata": {
        "id": "vI7TBw85vfAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Main Processing Pipeline\n",
        "def run_huggingface_pipeline(manual_paths: List[str]) -> HuggingFaceChatbot:\n",
        "    \"\"\"Run the complete processing pipeline with Hugging Face\"\"\"\n",
        "\n",
        "    print(\"Starting Hugging Face processing pipeline...\")\n",
        "\n",
        "    # Step 1: Load and process PDFs\n",
        "    print(\"Step 1: Loading PDFs...\")\n",
        "    documents = load_and_process_pdfs(manual_paths)\n",
        "\n",
        "    # Check if we have valid documents\n",
        "    valid_docs = [doc for doc in documents if not doc['metadata'].get('error', False)]\n",
        "\n",
        "    if not valid_docs:\n",
        "        print(\"No valid documents loaded. Cannot proceed with pipeline.\")\n",
        "        return None\n",
        "    elif len(valid_docs) < len(documents):\n",
        "        print(f\"Only {len(valid_docs)}/{len(documents)} documents loaded successfully.\")\n",
        "        documents = valid_docs\n",
        "\n",
        "    # Step 2: Create chunks\n",
        "    print(f\"Step 2: Creating chunks from {len(documents)} valid pages...\")\n",
        "    chunks = create_semantic_chunks(documents, chunk_size=800, overlap=150)\n",
        "\n",
        "    if not chunks:\n",
        "        print(\"No chunks created. Cannot proceed.\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Setup vector store\n",
        "    print(f\"Step 3: Setting up vector store with {len(chunks)} chunks...\")\n",
        "    try:\n",
        "        vector_store = SimpleVectorStore(embedding_model)\n",
        "        vector_store.add_documents(chunks)\n",
        "    except Exception as e:\n",
        "        print(f\"Vector store setup failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Step 4: Initialize chatbot\n",
        "    print(\"Step 4: Initializing Hugging Face chatbot...\")\n",
        "    try:\n",
        "        chatbot = HuggingFaceChatbot(qa_pipeline, vector_store)\n",
        "    except Exception as e:\n",
        "        print(f\"Chatbot initialization failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    print(\"Pipeline complete! Chatbot ready for use.\")\n",
        "\n",
        "    # Display summary\n",
        "    successful_manuals = len(set(doc['metadata'].get('document_name', 'Unknown') for doc in documents))\n",
        "    print(f\"\"\"\n",
        "PROCESSING SUMMARY:\n",
        "- Manual files processed: {len(manual_paths)}\n",
        "- Successfully loaded manuals: {successful_manuals}\n",
        "- Total pages processed: {len(documents)}\n",
        "- Total chunks created: {len(chunks)}\n",
        "- Vector store: Ready\n",
        "- Chatbot: Initialized\n",
        "\n",
        "Ready to answer questions about your manuals!\n",
        "\"\"\")\n",
        "\n",
        "    return chatbot\n",
        "\n",
        "# Execute pipeline\n",
        "if manual_paths:\n",
        "    print(f\"Found {len(manual_paths)} PDF files to process:\")\n",
        "    for i, path in enumerate(manual_paths, 1):\n",
        "        print(f\"   {i}. {path}\")\n",
        "\n",
        "    chatbot = run_huggingface_pipeline(manual_paths)\n",
        "\n",
        "    if chatbot:\n",
        "        print(\"SUCCESS! Your Hugging Face chatbot is ready!\")\n",
        "    else:\n",
        "        print(\"FAILED! Please check the error messages above.\")\n",
        "else:\n",
        "    print(\"No PDF files found. Please upload PDF files first.\")\n",
        "    chatbot = None"
      ],
      "metadata": {
        "id": "eEW72JDEwZwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Example Interactions\n",
        "def run_example_interactions(chatbot: HuggingFaceChatbot):\n",
        "    \"\"\"Run example interactions\"\"\"\n",
        "\n",
        "    print(\"RUNNING EXAMPLE INTERACTIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    examples = [\n",
        "        {\n",
        "            \"type\": \"Samsung Galaxy S23 Setup\",\n",
        "            \"session\": \"demo_1\",\n",
        "            \"question\": \"How do I set up my Samsung Galaxy S23 for the first time?\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Canon Photography\",\n",
        "            \"session\": \"demo_2\",\n",
        "            \"question\": \"What are the different shooting modes on the Canon EOS Rebel T7?\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Whirlpool Washing\",\n",
        "            \"session\": \"demo_3\",\n",
        "            \"question\": \"How do I select wash cycles on the Whirlpool washing machine?\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Battery Management\",\n",
        "            \"session\": \"demo_4\",\n",
        "            \"question\": \"How can I optimize battery life on my Samsung Galaxy S23?\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Camera Settings\",\n",
        "            \"session\": \"demo_5\",\n",
        "            \"question\": \"How do I adjust ISO settings on the Canon T7?\",\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, example in enumerate(examples, 1):\n",
        "        print(f\"\\n{i}. {example['type']}\")\n",
        "        print(f\"Question: {example['question']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        result = chatbot.chat(\n",
        "            session_id=example['session'],\n",
        "            user_input=example['question'],\n",
        "            show_sources=True\n",
        "        )\n",
        "\n",
        "        print(f\"Response: {result['response']}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "# Run examples if chatbot is available\n",
        "if 'chatbot' in locals() and chatbot:\n",
        "    run_example_interactions(chatbot)\n"
      ],
      "metadata": {
        "id": "Qw9Mi2jwxMrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Pre-filled Interactive Multi-turn Demo\n",
        "def interactive_demo_pre_filled(chatbot):\n",
        "    \"\"\"\n",
        "    Runs pre-filled demo for multiple sessions:\n",
        "    - Direct factual questions\n",
        "    - Memory-based follow-ups\n",
        "    - Out-of-scope question\n",
        "    \"\"\"\n",
        "    sessions = {\n",
        "        \"s25_demo\": [\n",
        "            \"How do I set up my Samsung Galaxy S25 for the first time?\",\n",
        "            \"How can I optimize battery life on my Samsung Galaxy S25?\"\n",
        "        ],\n",
        "        \"canon_demo\": [\n",
        "            \"What are the different shooting modes on the Canon EOS Rebel T7?\",\n",
        "            \"How do I adjust ISO settings for night photography on the Canon T7?\"\n",
        "        ],\n",
        "        \"out_of_scope_demo\": [\n",
        "            \"What is the capital of France?\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for session_id, questions in sessions.items():\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Session: {session_id} (demonstrating memory and retrieval)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for question in questions:\n",
        "            print(f\"\\nYou: {question}\")\n",
        "            result = chatbot.chat(session_id, question, show_sources=True)\n",
        "            print(f\"Chatbot: {result['response']}\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "# Run the pre-filled demo if chatbot is ready\n",
        "if 'chatbot' in locals() and chatbot:\n",
        "    interactive_demo_pre_filled(chatbot)\n",
        "else:\n",
        "    print(\"Chatbot not initialized. Please run the main pipeline first.\")\n"
      ],
      "metadata": {
        "id": "UWMq6Bph_uhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}